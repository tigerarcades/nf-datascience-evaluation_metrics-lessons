{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Data Science Classification Metrics in Sci-kit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through a few of few classifications metrics in Python's scikit-learn and write our own functions from scratch to understand the math behind a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major area of predictive modeling in data science is classification. Classification consists of trying to predict which class a particular sample from a population comes from. For example, if we are trying to predict if a particular patient will be re-hospitalized, the two possible classes are hospital (positive) and not-hospitalized (negative). The classification model then tries to predict if each patient will be hospitalized or not hospitalized. In other words, classification is simply trying to predict which bucket (predicted positive vs predicted negative) a particular sample from the population should be placed as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/classification.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you train your classification predictive model, you will want to assess how good it is. Interestingly, there are many different ways of evaluating the performance. Most data scientists that use Python for predictive modeling use the Python package called scikit-learn. Scikit-learn contains many built-in functions for analyzing the performance of models. \n",
    "In this tutorial, we will walk through a few of these metrics and write our own functions from scratch to understand the math behind a few of them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will cover the following metrics functions from `sklearn.metrics` :\n",
    "    - confusion_matrix\n",
    "    - accuracy_score\n",
    "    - recall_score\n",
    "    - precision_score\n",
    "    - f1_score\n",
    "    - roc_curve\n",
    "    - roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will write our own functions from scratch assuming a two-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a sample data set that has the actual labels (actual_label) and the prediction probabilities for two models (model_RF and model_LR). Here the probabilities are the probability of being class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most data science projects, you will define a threshold to define which prediction probabilities are labeled as predicted positive vs predicted negative. For now let's assume the threshold is 0.5. Let's add two additional columns that convert the probabilities to predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "df['predicted_RF'] = (df.model_RF >= 0.5).astype('int')\n",
    "df['predicted_LR'] = (df.model_LR >= 0.5).astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an actual label and a predicted label, the first thing we can do is divide our samples in 4 buckets:\n",
    "    - True positive - actual = 1, predicted = 1\n",
    "    - False positive - actual = 1, predicted = 0\n",
    "    - False negative - actual = 0, predicted = 1\n",
    "    - True negative - actual = 0, predicted = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These buckets can be represented with the following image (original source https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg) and we will reference this image in many of the calculations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/buckets.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These buckets can also be displayed using a confusion matrix as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/conf_matrix.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the confusion matrix (as a 2x2 array) from scikit learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where there were 5047 true positives, 2360 false positives, 2832 false negatives and 5519 true negatives. Let's define our own functions to verify `confusion_matrix`. Note that I filled in the first one and you need to fill in the other 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_TP(y_true, y_pred):\n",
    "    # counts the number of true positives (y_true = 1, y_pred = 1)\n",
    "    return sum((y_true == 1) & (y_pred == 1))\n",
    "def find_FN(y_true, y_pred):\n",
    "    # counts the number of false negatives (y_true = 1, y_pred = 0)\n",
    "    return # your code here\n",
    "def find_FP(y_true, y_pred):\n",
    "    # counts the number of false positives (y_true = 0, y_pred = 1)\n",
    "    return # your code here\n",
    "def find_TN(y_true, y_pred):\n",
    "    # counts the number of true negatives (y_true = 0, y_pred = 0)\n",
    "    return # your code here\n",
    "\n",
    "print('TP:',find_TP(df.actual_label.values, df.predicted_RF.values))\n",
    "print('FN:',find_FN(df.actual_label.values, df.predicted_RF.values))\n",
    "print('FP:',find_FP(df.actual_label.values, df.predicted_RF.values))\n",
    "print('TN:',find_TN(df.actual_label.values, df.predicted_RF.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that will calculate all four of these for us, and another function to duplicate `confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_conf_matrix_values(y_true,y_pred):\n",
    "    # calculate TP, FN, FP, TN\n",
    "    TP = find_TP(y_true,y_pred)\n",
    "    FN = find_FN(y_true,y_pred)\n",
    "    FP = find_FP(y_true,y_pred)\n",
    "    TN = find_TN(y_true,y_pred)\n",
    "    return TP,FN,FP,TN\n",
    "def my_confusion_matrix(y_true, y_pred):\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)\n",
    "    return np.array([[TN,FP],[FN,TP]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_confusion_matrix(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our functions worked using Python's built in `assert` and numpy's `array_equal` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert  np.array_equal(my_confusion_matrix(df.actual_label.values, df.predicted_RF.values),\\\n",
    "                       confusion_matrix(df.actual_label.values, df.predicted_RF.values) ), 'my_confusion_matrix() is not correct for RF'\n",
    "\n",
    "assert  np.array_equal(my_confusion_matrix(df.actual_label.values, df.predicted_LR.values),\\\n",
    "                       confusion_matrix(df.actual_label.values, df.predicted_LR.values) ), 'my_confusion_matrix() is not correct for LR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these four buckets (TP, FP, FN, TN), we can calculate many other performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/accuracy.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the accuracy score from scikit learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own function that duplicates `accuracy_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_accuracy_score(y_true, y_pred):\n",
    "    # calculates the fraction of samples predicted correctly\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n",
    "    return # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert my_accuracy_score(df.actual_label.values, df.predicted_RF.values) == accuracy_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_accuracy_score(df.actual_label.values, df.predicted_LR.values) == accuracy_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('Accuracy RF: %.3f'%(my_accuracy_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Accuracy LR: %.3f'%(my_accuracy_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using accuracy as a performance metric, the RF model is more accurate than the LR model. So should we stop here and say RF model is the best model? No! Accuracy is not always the best metric to use to assess classification models. For example, let's say that we are trying to predict something that only happens 1 out of 100 times. We could build a model that gets 99% accuracy by saying the event never happened. However, we catch 0% of the events we care about. The 0% measure here is another performance metric known as recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/recall.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own function that duplicates `recall_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_recall_score(y_true, y_pred):\n",
    "    # calculates the fraction of positive samples predicted correctly\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n",
    "    return # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert my_recall_score(df.actual_label.values, df.predicted_RF.values) == recall_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_recall_score(df.actual_label.values, df.predicted_LR.values) == recall_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('Recall RF: %.3f'%(my_recall_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Recall LR: %.3f'%(my_recall_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method to boost the recall is to increase the number of samples that you define as predicted positive by lowering the threshold for predicted positive. Unfortunately, this will also increase the number of false positives. Another performance metric called precision takes this into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the fraction of predicted positives events that are actually positive as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/precision.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own function that duplicates `precision_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_precision_score(y_true, y_pred):\n",
    "    # calculates the fraction of predicted positives samples that are actually positive\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n",
    "    return # your code here\n",
    "\n",
    "assert my_precision_score(df.actual_label.values, df.predicted_RF.values) == precision_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_precision_score(df.actual_label.values, df.predicted_LR.values) == precision_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Precision LR: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it looks like RF model is better at both recall and precision. But what would you do if one model was better at recall and the other was better at precision. One method that some data scientists use is called the F1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/f1_score.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the f1 score from scikit-learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own function that duplicates `f1_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "    # calculates the F1 score\n",
    "    recall = my_recall_score(y_true,y_pred)  \n",
    "    precision = my_precision_score(y_true,y_pred)  \n",
    "    return # your code here\n",
    "\n",
    "assert my_f1_score(df.actual_label.values, df.predicted_RF.values) == f1_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_f1_score(df.actual_label.values, df.predicted_LR.values) == f1_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('F1 LR: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have assumed that we defined a threshold of 0.5 for selecting which samples are predicted as positive. If we change this threshold the performance metrics will change. As shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('scores with threshold = 0.5')\n",
    "print('Accuracy RF: %.3f'%(my_accuracy_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Recall RF: %.3f'%(my_recall_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print(' ')\n",
    "print('scores with threshold = 0.25')\n",
    "print('Accuracy RF: %.3f'%(my_accuracy_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('Recall RF: %.3f'%(my_recall_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we assess a model if we haven't picked a threshold? One very common method is using the receiver operating characteristic (ROC) curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roc_curve and roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curves are VERY help with understanding the balance between true-positive rate and false positive rates. Sci-kit learn has built in functions for ROC curves and for analyzing them. The inputs to these functions (`roc_curve` and `roc_auc_score`) are the actual labels and the predicted probabilities (not the predicted labels).  Both `roc_curve` and `roc_auc_score` are both complicated functions, so we will not have you write these functions from scratch. Instead, we will show you how to use sci-kit learn's functions and explain the key points. Let's begin by using `roc_curve` to make the ROC plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_RF, tpr_RF, thresholds_RF = roc_curve(df.actual_label.values, df.model_RF.values)\n",
    "fpr_LR, tpr_LR, thresholds_LR = roc_curve(df.actual_label.values, df.model_LR.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `roc_curve` function returns three lists:\n",
    "    - thresholds = all unique prediction probabilities in descending order\n",
    "    - fpr = the false positive rate (FP / (FP+TN)) for each threshold\n",
    "    - tpr = the true positive rate  (TP / (TP+FN)) (i.e. recall) for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpr_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the ROC curve for each model as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_RF, tpr_RF,'r-',label = 'RF')\n",
    "plt.plot(fpr_LR,tpr_LR,'b-', label= 'LR')\n",
    "plt.plot([0,1],[0,1],'k-',label='random')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple things that we can observe from this figure. \n",
    "    - A model that randomly guesses the label will result in the black line and you want to have a model that has a curve above this black line. \n",
    "    - An ROC that is farther away from the black line is better, so RF (red) looks better than LR (blue).\n",
    "    - Although not seen directly, a high threshold results in a point in the top right and a low threshold results in a point in the bottom left. This means as you increase the threshold you get higher TPR at a cost of higher FPR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the performance, we will use the area-under-curve metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_RF = roc_auc_score(df.actual_label.values, df.model_RF.values)\n",
    "auc_LR = roc_auc_score(df.actual_label.values, df.model_LR.values)\n",
    "\n",
    "print('AUC RF:%.3f'% auc_RF)\n",
    "print('AUC LR:%.3f'% auc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the area under the curve for the RF model is better than the LR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I plot the ROC curve, I like to add the AUC to the legend as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr_RF, tpr_RF,'r-',label = 'RF AUC: %.3f'%auc_RF)\n",
    "plt.plot(fpr_LR,tpr_LR,'b-', label= 'LR AUC: %.3f'%auc_LR)\n",
    "plt.plot([0,1],[0,1],'k-',label='random')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, in this toy example the model RF wins with every performance metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In predictive analytics, when deciding between two models it is important to pick a single performance metric. As you can see here, there are many that you can choose from (accuracy, recall, precision, f1-score, AUC, etc). Ultimately, you should use the perfomance metric that is most suitable for the business problem at hand. Many data scientists prefer to use the AUC because it does not require selecting a threshold and helps balance true positive rate and false positive rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "NEPA",
   "language": "python",
   "name": "nepa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
